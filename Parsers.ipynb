{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Была госпитализирована в Тоскенскую ЦРБ с диагнозом ОИМ.\",\n",
    "    \"18.08.11 обратилась в поликлинику с жалобами на длительные жгучие боли в области сердца, одышку.\",\n",
    "    'На фоне проводимого лечения сохранялись боли в области сердца в связи с чем была направлена на госпитализацию в федеральное государственное бюджетное учреждение \"Федеральный центр сердца крови и эндокринологии им Алмазова\".',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"ru_syntagrus-ud-test.conllu\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "pattern = \"text = (.*?)\\n1\"\n",
    "test_sentences = re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "sentences_7777 = list()\n",
    "dataset_file = pathlib.Path(r\"Data/separeted_sentences_txt_extra_7777\")\n",
    "docs = [file for file in dataset_file.iterdir() if file.suffix == \".txt\"]\n",
    "docs.sort()\n",
    "\n",
    "for doc in docs:\n",
    "    with open(doc, 'r') as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('\\n', '')\n",
    "        sentences_7777.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qbic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/AlexeySorokin/GramEval2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/DanAnastasyev/GramEval2020.git\n",
    "!cd GramEval2020/ && pip install -r requirements.txt && ./download_data.sh\n",
    "!pip install git+git://github.com/DanAnastasyev/allennlp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd GramEval2020 \\\n",
    "    && ./download_model.sh trainable_bert_morph_lstm \\\n",
    "    && cd solution \\\n",
    "    && python -m train.applier --model-name trainable_bert_morph_lstm --batch-size 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо поменять пути до нужных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!cd GramEval2020/solution \\\n",
    "    && python -m train.applier \\\n",
    "        --pretrained-models-dir /home/renoip/Jupyter/Parsers/qbic/GramEval2020/pretrained_models \\\n",
    "        --models-dir /home/renoip/Jupyter/Parsers/qbic/GramEval2020/models \\\n",
    "        --model-name trainable_bert_morph_lstm --batch-size 8 \\\n",
    "        --data-dir /home/renoip/Jupyter/Parsers/qbic/Data \\\n",
    "        --predictions-dir /home/renoip/Jupyter/Parsers/qbic/Preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time - 1m 58s (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |     98.79 |     98.37 |     98.58 |\n",
      "Sentences  |    100.00 |    100.00 |    100.00 |\n",
      "Words      |     98.79 |     98.37 |     98.58 |\n",
      "UPOS       |     97.64 |     97.22 |     97.43 |     98.83\n",
      "XPOS       |     98.79 |     98.37 |     98.58 |    100.00\n",
      "UFeats     |     95.00 |     94.60 |     94.80 |     96.17\n",
      "AllTags    |     94.16 |     93.76 |     93.96 |     95.32\n",
      "Lemmas     |     97.68 |     97.26 |     97.47 |     98.88\n",
      "UAS        |     97.18 |     96.77 |     96.98 |     98.38\n",
      "LAS        |     94.36 |     93.96 |     94.16 |     95.52\n",
      "CLAS       |     92.84 |     92.62 |     92.73 |     94.36\n",
      "MLAS       |     86.80 |     86.59 |     86.69 |     88.22\n",
      "BLEX       |     91.56 |     91.33 |     91.45 |     93.05\n"
     ]
    }
   ],
   "source": [
    "!python conll18_ud_eval.py -v ru_syntagrus-ud-test.conllu qbic_test.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* UAS - 96.98\n",
    "* LAS - 94.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "использовал токенизацию от Slovnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://turkunlp.org/Turku-neural-parser-pipeline/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно установить используя docker контейнер http://turkunlp.org/Turku-neural-parser-pipeline/docker.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time - 5min (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |     99.60 |     99.60 |     99.60 |\n",
      "Sentences  |     97.85 |     98.26 |     98.06 |\n",
      "Words      |     99.60 |     99.60 |     99.60 |\n",
      "UPOS       |     97.72 |     97.73 |     97.72 |     98.12\n",
      "XPOS       |     99.60 |     99.60 |     99.60 |    100.00\n",
      "UFeats     |     96.19 |     96.20 |     96.20 |     96.59\n",
      "AllTags    |     95.43 |     95.43 |     95.43 |     95.82\n",
      "Lemmas     |     94.30 |     94.31 |     94.30 |     94.68\n",
      "UAS        |     88.86 |     88.87 |     88.86 |     89.22\n",
      "LAS        |     87.08 |     87.09 |     87.08 |     87.43\n",
      "CLAS       |     89.78 |     89.65 |     89.71 |     90.08\n",
      "MLAS       |     84.91 |     84.79 |     84.85 |     85.20\n",
      "BLEX       |     83.03 |     82.91 |     82.97 |     83.31\n"
     ]
    }
   ],
   "source": [
    "!python conll18_ud_eval.py -v ru_syntagrus-ud-test.conllu Turku_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* UAS - 88.86\n",
    "* LAS - 87.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит отметить, что парсер сам выполнял разбиение на предложения, в отличии от готового разбиения в других парсерах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/buriy/spacy-ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для модели spacy для русского языка требуется pymorphy2 версии 0.8 и spacy версии 2.1.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymorphy2==0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy==2.1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b v2.1 https://github.com/buriy/spacy-ru.git && cp -r ./spacy-ru/ru2/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"spacy-ru/ru2\")\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for sent in sentences:\n",
    "    doc = nlp(sent)\n",
    "    print(sent, end=\"\\n\\n\")\n",
    "    [to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "    print(\"*\" * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение результата в формате Conll-U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_conll import ConllFormatter\n",
    "\n",
    "nlp = spacy.load(\"spacy-ru/ru2\")\n",
    "conllformatter = ConllFormatter(nlp)\n",
    "nlp.add_pipe(conllformatter, after='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"Spacy_7777.txt\", \"w\") as f:\n",
    "    for sent in range(len(sentences_7777)):\n",
    "        doc = nlp(sentences_7777[sent])\n",
    "        sent_id = str(docs[sent])\n",
    "        print(\"# sent_id = \" + sent_id[sent_id.rfind(\"/\") + 1 :], file=f)\n",
    "        print(\"# text = \" + sentences_7777[sent], file=f)\n",
    "        for i in range(len(doc)):\n",
    "            token = doc[i]\n",
    "            if token.dep_ =='root':\n",
    "                dep = 0\n",
    "            else: dep = token.head.i+1\n",
    "            print('{}\\t{}\\t{}\\t{}\\t_\\t_\\t{}\\t{}\\t_\\t_'.format(i+1, token.text, token.lemma_.strip(), token.pos_, dep, token.dep_), file=f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"Spacy_test.txt\", \"w\") as f:\n",
    "    for sent in range(len(test_sentences)):\n",
    "        doc = nlp(test_sentences[sent])\n",
    "\n",
    "        print(\"# sent_id = \" + str(sent), file=f)\n",
    "        print(\"# text = \" + test_sentences[sent], file=f)\n",
    "        for i in range(len(doc)):\n",
    "            token = doc[i]\n",
    "            if token.dep_ =='root':\n",
    "                dep = 0\n",
    "            else: dep = token.head.i+1\n",
    "            print('{}\\t{}\\t{}\\t{}\\t_\\t_\\t{}\\t{}\\t_\\t_'.format(i+1, token.text, token.lemma_.strip(), token.pos_, dep, token.dep_), file=f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time - 2min 39s (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |     97.91 |     98.70 |     98.31 |\n",
      "Sentences  |    100.00 |    100.00 |    100.00 |\n",
      "Words      |     97.91 |     98.70 |     98.31 |\n",
      "UPOS       |     95.39 |     96.16 |     95.77 |     97.42\n",
      "XPOS       |      0.00 |      0.00 |      0.00 |      0.00\n",
      "UFeats     |     91.38 |     92.12 |     91.75 |     93.33\n",
      "AllTags    |      0.00 |      0.00 |      0.00 |      0.00\n",
      "Lemmas     |     86.58 |     87.28 |     86.93 |     88.42\n",
      "UAS        |     83.12 |     83.80 |     83.46 |     84.90\n",
      "LAS        |     75.26 |     75.87 |     75.56 |     76.86\n",
      "CLAS       |     81.48 |     74.43 |     77.80 |     75.60\n",
      "MLAS       |     73.84 |     67.45 |     70.50 |     68.51\n",
      "BLEX       |     68.71 |     62.77 |     65.61 |     63.75\n"
     ]
    }
   ],
   "source": [
    "!python conll18_ud_eval.py -v ru_syntagrus-ud-test.conllu Spacy_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* UAS - 86.93\n",
    "* LAS - 75.56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://docs.deeppavlov.ai/en/master/features/models/syntaxparser.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov install syntax_ru_syntagrus_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение результата в формате Conll-U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не чувствителен к пунктуации. Не использует морфологические метки. Параметры Bert модели открыты и можно попробовать их подредактировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from deeppavlov import build_model, configs\n",
    "\n",
    "sent_id = 0\n",
    "model = build_model(configs.syntax.syntax_ru_syntagrus_bert, download=True)\n",
    "\n",
    "sentences = ['С 31.01.11 г до 17.02.11 г находилась на лечении КБ № 81 гСеверска с диагнозом, Осн, ИБС, ОИМ в области нижней стенки левого желудочка, Q образующий от 28.01.2011 г']\n",
    "for parse in model(sentences):\n",
    "        print(parse, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from deeppavlov import build_model, configs\n",
    "\n",
    "sent_id = 0\n",
    "model = build_model(\"ru_syntagrus_joint_parsing\", download=True)#(configs.syntax.syntax_ru_syntagrus_bert, download=True)\n",
    "with open(\"DeepPavlov_7777.txt\", \"w\") as f:\n",
    "    for sent in range(len(sentences_7777)//3+1):\n",
    "        try:\n",
    "            for parse in model(sentences_7777[sent*3:(sent+1)*3]):\n",
    "                print(\"# sent_id = \" + str(sent_id), file=f)\n",
    "                print(\"# text = \" + sentences_7777[sent_id], file=f)\n",
    "                print(parse, file=f)\n",
    "                print(file=f)\n",
    "                sent_id += 1\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in range(len(sentences_7777)):\n",
    "    if sentences_7777[sent][-1] != '.':\n",
    "        sentences_7777[sent] = sentences_7777[sent] + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from deeppavlov import build_model, configs\n",
    "\n",
    "sent_id = 0\n",
    "model = build_model(configs.syntax.syntax_ru_syntagrus_bert, download=True)\n",
    "with open(\"DeepPavlov_test.txt\", \"w\") as f:\n",
    "    for sent in range(len(test_sentences)//3+1):\n",
    "        try:\n",
    "            for parse in model(test_sentences[sent*3:(sent+1)*3]):\n",
    "                print(\"# sent_id = \" + str(sent_id), file=f)\n",
    "                print(\"# text = \" + test_sentences[sent_id], file=f)\n",
    "                print(parse, file=f)\n",
    "                sent_id += 1\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time - 2 min 3s (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4178657"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('DeepPavlov_test.txt', 'r')\n",
    "text = f.read()\n",
    "text = text.replace(\"''\", '\"')\n",
    "text = text.replace(\"``\", '\"')\n",
    "f = open('DeepPavlov_test.txt', 'w')\n",
    "f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |     99.32 |     98.43 |     98.88 |\n",
      "Sentences  |    100.00 |    100.00 |    100.00 |\n",
      "Words      |     99.32 |     98.43 |     98.88 |\n",
      "UPOS       |      0.00 |      0.00 |      0.00 |      0.00\n",
      "XPOS       |     99.32 |     98.43 |     98.88 |    100.00\n",
      "UFeats     |     37.82 |     37.49 |     37.65 |     38.08\n",
      "AllTags    |      0.00 |      0.00 |      0.00 |      0.00\n",
      "Lemmas     |      0.00 |      0.00 |      0.00 |      0.00\n",
      "UAS        |     93.82 |     92.98 |     93.40 |     94.46\n",
      "LAS        |     92.49 |     91.66 |     92.07 |     93.12\n",
      "CLAS       |     91.85 |     91.04 |     91.45 |     92.50\n",
      "MLAS       |      0.00 |      0.00 |      0.00 |      0.00\n",
      "BLEX       |      0.00 |      0.00 |      0.00 |      0.00\n"
     ]
    }
   ],
   "source": [
    "!python conll18_ud_eval.py -v ru_syntagrus-ud-test.conllu DeepPavlov_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* UAS - 93.40\n",
    "* LAS - 92.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slovnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/natasha/slovnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже интегрирован в Natasha, но позволяет использовать выходной формат CONLL-U, а также можно попробовать другие настройки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install slovnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install navec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipymarkup import show_dep_markup as show_markup\n",
    "from navec import Navec\n",
    "from razdel import sentenize, tokenize\n",
    "from slovnet import Syntax\n",
    "\n",
    "navec = Navec.load(\"navec_news_v1_1B_250K_300d_100q.tar\")\n",
    "syntax = Syntax.load(\"slovnet_syntax_news_v1.tar\")\n",
    "syntax.navec(navec)\n",
    "\n",
    "for sent in sentences:\n",
    "    print(sent, end=\"\\n\\n\")\n",
    "    chunk = []\n",
    "    tokens = [_.text for _ in tokenize(sent)]\n",
    "    chunk.append(tokens)\n",
    "\n",
    "    markup = next(syntax.map(chunk))\n",
    "    words, deps = [], []\n",
    "    for token in markup.tokens:\n",
    "        words.append(token.text)\n",
    "        source = int(token.head_id) - 1\n",
    "        target = int(token.id) - 1\n",
    "        if source > 0 and source != target:\n",
    "            deps.append([source, target, token.rel])\n",
    "    show_markup(words, deps)\n",
    "    print(\"*\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"Slovnet_7777.txt\", \"w\") as f:\n",
    "    for sent in range(len(sentences_7777)):\n",
    "        chunk = []\n",
    "        tokens = [_.text for _ in tokenize(sentences_7777[sent])]\n",
    "        chunk.append(tokens)\n",
    "\n",
    "        markup = next(syntax.map(chunk))\n",
    "        \n",
    "        print(\"# sent_id = \" + str(sent), file=f)\n",
    "        print(\"# text = \" + sentences_7777[sent], file=f)\n",
    "        for i in markup.as_json['tokens']:\n",
    "            print('{}\\t{}\\t_\\t_\\t_\\t_\\t{}\\t{}\\t_\\t_'.format(i['id'], i['text'], i['head_id'], i['rel']), file=f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['В апреле 2010 года проходила лечение в отд НРС, проводилась РЧ-аблация желудочковой экстрасистолии - успешно, на контрольном холтере за сутки была 1 экстрасистола.',\n",
    "             '2 раза в день, эгилок 50 мг 2 раза в день.',\n",
    "             'Около 5 лет назад выявлен СД 2 типа, подагра.',\n",
    "             'Около 5 лет назад выявлен СД второго типа, подагра.',\n",
    "             'В последствии, в 1995 году выявлен эписиндром, с 1995 года принимает бензонал 1 таб вечером ежедневно, последний приступ в 1997 году.',\n",
    "             'В апреле 2009 госпитализация в отделение ОСН, где проведено КВГ, стенозирующего атеросклероза коронарных артерий не выявлено. ',\n",
    "             'После операции через 1-2 месяца количество пароксизмов уменьшилось.',\n",
    "             'Длительность приступов до 2 часов, частота до нескольких раз в месяц. ',\n",
    "             'Длительность приступов до двух часов, частота до нескольких раз в месяц. ',\n",
    "             'Последняя госпитализация в НРС в феврале 2010 года, проводилось СМ ЭКГ – усредненная ЧСС - 71 в минуту.',\n",
    "             'В последние несколько месяцев отмечает нарастание одышки, появились жалобы на учащенное сердцебиение. ',\n",
    "             'Язвенная болезнь, язва желудка, стадия ремиссии, последнее обострение 1993 год. ',\n",
    "             'Последний раз ФГДС в 2008 году - рубцевание.',\n",
    "             'Обследовалась по м/ж, на ЭХО-КГ выявлено расширение ЛП, по данным УЗИ - диффузные изменения ЩЖ, гормоны ЩЖ - в пределах нормы (от 11.2010). ',\n",
    "             'У сестры - открытый Боталлов проток сестра - ГБ мать - ГБ, умерла от ОНМК в 71',\n",
    "             'Больной себя считает в течении последних 15 лет, когда впервые стала отмечтаь повышение АД, эпизоды слабости, к рачарм не обращалась, считая, что данные проявления связаны с психоэмоциональными перегрузками около 10 лет назад стала отмечать появление давящих болей за грудиной, повышение АД до цифр 200\\100 мм.рт.ст.',\n",
    "             'Нарушение ритма в течении 4х лет.',\n",
    "             'Нарушение ритма в течении 4 лет.',\n",
    "             'Весной 2010 года проконсультирована аритмологом рекомендована имплантация ЭКС в НИИ Кардиологии. ',\n",
    "            'Весной 2010 года проконсультирована аритмологом, рекомендована имплантация ЭКС в НИИ Кардиологии.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 270 ms, sys: 614 ms, total: 884 ms\n",
      "Wall time: 42.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"words_errors.conllu\", \"w\") as f:\n",
    "    for sent in range(len(sentences)):\n",
    "        chunk = []\n",
    "        tokens = [_.text for _ in tokenize(sentences[sent])]\n",
    "        chunk.append(tokens)\n",
    "\n",
    "        markup = next(syntax.map(chunk))\n",
    "        \n",
    "#         print(\"# sent_id = \" + str(sent), file=f)\n",
    "#         print(\"# text = \" + sentences_7777[sent], file=f)\n",
    "        for i in markup.as_json['tokens']:\n",
    "            print('{}\\t{}'.format(i['id'], i['text']), file=f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"Syntagrus_test.txt\", \"w\") as f:\n",
    "    for sent in range(len(test_sentences)):\n",
    "        chunk = []\n",
    "        tokens = [_.text for _ in tokenize(test_sentences[sent])]\n",
    "        chunk.append(tokens)\n",
    "\n",
    "        markup = next(syntax.map(chunk))\n",
    "        \n",
    "        print(\"# sent_id = \" + str(sent), file=f)\n",
    "        print(\"# text = \" + test_sentences[sent], file=f)\n",
    "        for i in markup.as_json['tokens']:\n",
    "            print('{}\\t{}\\t_\\t_\\t_\\t_\\t{}\\t{}\\t_\\t_'.format(i['id'], i['text'], i['head_id'], i['rel']), file=f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time - 9.19 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |     98.79 |     98.37 |     98.58 |\n",
      "Sentences  |    100.00 |    100.00 |    100.00 |\n",
      "Words      |     98.79 |     98.37 |     98.58 |\n",
      "UPOS       |      0.00 |      0.00 |      0.00 |      0.00\n",
      "XPOS       |     98.79 |     98.37 |     98.58 |    100.00\n",
      "UFeats     |     37.77 |     37.61 |     37.69 |     38.23\n",
      "AllTags    |      0.00 |      0.00 |      0.00 |      0.00\n",
      "Lemmas     |      0.00 |      0.00 |      0.00 |      0.00\n",
      "UAS        |     80.07 |     79.73 |     79.90 |     81.05\n",
      "LAS        |     75.50 |     75.18 |     75.34 |     76.43\n",
      "CLAS       |     72.80 |     72.37 |     72.59 |     73.73\n",
      "MLAS       |      0.00 |      0.00 |      0.00 |      0.00\n",
      "BLEX       |      0.00 |      0.00 |      0.00 |      0.00\n"
     ]
    }
   ],
   "source": [
    "!python conll18_ud_eval.py -v ru_syntagrus-ud-test.conllu Slovnet_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* UAS - 81.05\n",
    "* LAS - 76.43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/TakeLab/spacy-udpipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy-udpipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_udpipe\n",
    "from nltk import Tree\n",
    "\n",
    "nlp = spacy_udpipe.load_from_path(\n",
    "    lang=\"ru\",\n",
    "    path=\"./russian-syntagrus-ud-2.5-191206.udpipe\",\n",
    "    meta={\"description\": \"Syntagrus\"},\n",
    ")\n",
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for sent in sentences:\n",
    "    doc = nlp(sent)\n",
    "    print(sent, end=\"\\n\\n\")\n",
    "    [to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "    print('*'*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение результата в формате Conll-U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_conll import ConllFormatter\n",
    "\n",
    "nlp = spacy_udpipe.load_from_path(\n",
    "    lang=\"ru\",\n",
    "    path=\"./russian-syntagrus-ud-2.5-191206.udpipe\",\n",
    "    meta={\"description\": \"Syntagrus\"},\n",
    ")\n",
    "conllformatter = ConllFormatter(nlp)\n",
    "nlp.add_pipe(conllformatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"Udpipe_7777.txt\", \"w\") as f:\n",
    "    for sent in range(len(sentences_7777)):\n",
    "        doc = nlp(sentences_7777[sent])\n",
    "        sent_id = str(docs[sent])\n",
    "        print(\"# sent_id = \" + sent_id[sent_id.rfind(\"/\") + 1 :], file=f)\n",
    "        print(\"# text = \" + sentences_7777[sent], file=f)\n",
    "        for i in range(len(doc)):\n",
    "            token = doc[i]\n",
    "            if token.dep_ =='root':\n",
    "                dep = 0\n",
    "            else: dep = token.head.i+1\n",
    "            print('{}\\t{}\\t{}\\t{}\\t_\\t_\\t{}\\t{}\\t_\\t_'.format(i+1, token.text, token.lemma_.strip(), token.pos_, dep, token.dep_), file=f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"UDPipe_test.txt\", \"w\") as f:\n",
    "    for sent in range(len(test_sentences)):\n",
    "        doc = nlp(test_sentences[sent])\n",
    "\n",
    "        print(\"# sent_id = \" + str(sent), file=f)\n",
    "        print(\"# text = \" + test_sentences[sent], file=f)\n",
    "        for i in range(len(doc)):\n",
    "            token = doc[i]\n",
    "            if token.dep_ =='root':\n",
    "                dep = 0\n",
    "            else: dep = token.head.i+1\n",
    "            print('{}\\t{}\\t{}\\t{}\\t_\\t_\\t{}\\t{}\\t_\\t_'.format(i+1, token.text, token.lemma_.strip(), token.pos_, dep, token.dep_), file=f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time - 1min 14s (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |     99.56 |     99.63 |     99.60 |\n",
      "Sentences  |     99.19 |     99.60 |     99.39 |\n",
      "Words      |     99.56 |     99.63 |     99.60 |\n",
      "UPOS       |     97.75 |     97.82 |     97.78 |     98.18\n",
      "XPOS       |      0.00 |      0.00 |      0.00 |      0.00\n",
      "UFeats     |     38.28 |     38.30 |     38.29 |     38.44\n",
      "AllTags    |      0.00 |      0.00 |      0.00 |      0.00\n",
      "Lemmas     |     96.51 |     96.58 |     96.55 |     96.94\n",
      "UAS        |     87.65 |     87.71 |     87.68 |     88.03\n",
      "LAS        |     80.03 |     80.09 |     80.06 |     80.38\n",
      "CLAS       |     82.09 |     74.76 |     78.25 |     75.09\n",
      "MLAS       |      7.19 |      6.55 |      6.86 |      6.58\n",
      "BLEX       |     79.06 |     71.99 |     75.36 |     72.31\n"
     ]
    }
   ],
   "source": [
    "!python conll18_ud_eval.py -v ru_syntagrus-ud-test.conllu UDPipe_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* UAS - 87.68\n",
    "* LAS - 80.06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza (formerly StanfordNLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stanfordnlp.github.io/stanza/#getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy-stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('ru') # syntagrus is default model for Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-10 11:22:17 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
      "2020-08-10 11:22:17 INFO: Use device: gpu\n",
      "2020-08-10 11:22:17 INFO: Loading: tokenize\n",
      "2020-08-10 11:22:19 INFO: Loading: pos\n",
      "2020-08-10 11:22:20 INFO: Loading: lemma\n",
      "2020-08-10 11:22:20 INFO: Loading: depparse\n",
      "2020-08-10 11:22:21 INFO: Loading: ner\n",
      "2020-08-10 11:22:22 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from nltk import Tree\n",
    "from spacy_stanza import StanzaLanguage\n",
    "\n",
    "snlp = stanza.Pipeline(lang=\"ru\")\n",
    "nlp = StanzaLanguage(snlp)\n",
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for sent in sentences:\n",
    "    doc = nlp(sent)\n",
    "    print(sent, end=\"\\n\\n\")\n",
    "    [to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "    print('*'*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение результата в формате Conll-U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-10 11:22:23 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
      "2020-08-10 11:22:23 INFO: Use device: gpu\n",
      "2020-08-10 11:22:23 INFO: Loading: tokenize\n",
      "2020-08-10 11:22:23 INFO: Loading: pos\n",
      "2020-08-10 11:22:24 INFO: Loading: lemma\n",
      "2020-08-10 11:22:24 INFO: Loading: depparse\n",
      "2020-08-10 11:22:24 INFO: Loading: ner\n",
      "2020-08-10 11:22:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from spacy_conll import ConllFormatter\n",
    "\n",
    "snlp = stanza.Pipeline(lang=\"ru\")\n",
    "nlp = StanzaLanguage(snlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, CPU times: user 4.55 s, sys: 21.5 ms, total: 4.57 s\n",
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"Stanza_100.txt\", \"w\") as f:\n",
    "    for sent in range(len(sentences_7777))[:100]:\n",
    "        if sentences_7777[sent][-1] != '.':\n",
    "            sentences_7777[sent] = sentences_7777[sent] + '.'\n",
    "        doc = nlp(sentences_7777[sent])\n",
    "        sent_id = str(docs[sent])\n",
    "        print(\"# sent_id = \" + sent_id[sent_id.rfind(\"/\") + 1 :], file=f)\n",
    "        print(\"# text = \" + sentences_7777[sent], file=f)\n",
    "        for i in range(len(doc)):\n",
    "            token = doc[i]\n",
    "            if token.dep_ =='root':\n",
    "                dep = 0\n",
    "            else: dep = token.head.i+1\n",
    "            print('{}\\t{}\\t{}\\t{}\\t_\\t_\\t{}\\t{}\\t_\\t_'.format(i+1, token.text, token.lemma_.strip(), token.pos_, dep, token.dep_), file=f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"Stanza_test.txt\", \"w\") as f:\n",
    "    for sent in range(len(test_sentences)):\n",
    "        doc = nlp(test_sentences[sent])\n",
    "\n",
    "        print(\"# sent_id = \" + str(sent), file=f)\n",
    "        print(\"# text = \" + test_sentences[sent], file=f)\n",
    "        print(doc._.conll_str, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time - 5min 53s (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |     99.29 |     99.26 |     99.28 |\n",
      "Sentences  |     91.65 |     95.66 |     93.61 |\n",
      "Words      |     99.29 |     99.26 |     99.28 |\n",
      "UPOS       |     97.91 |     97.89 |     97.90 |     98.62\n",
      "XPOS       |      0.00 |      0.00 |      0.00 |      0.00\n",
      "UFeats     |     38.11 |     38.10 |     38.11 |     38.39\n",
      "AllTags    |      0.00 |      0.00 |      0.00 |      0.00\n",
      "Lemmas     |     97.23 |     97.21 |     97.22 |     97.93\n",
      "UAS        |     91.76 |     91.74 |     91.75 |     92.42\n",
      "LAS        |     89.98 |     89.95 |     89.97 |     90.62\n",
      "CLAS       |     88.82 |     88.62 |     88.72 |     89.30\n",
      "MLAS       |      6.90 |      6.88 |      6.89 |      6.93\n",
      "BLEX       |     86.59 |     86.39 |     86.49 |     87.05\n"
     ]
    }
   ],
   "source": [
    "!python conll18_ud_eval.py -v ru_syntagrus-ud-test.conllu Stanza_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* UAS - 91.75\n",
    "* LAS - 89.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
